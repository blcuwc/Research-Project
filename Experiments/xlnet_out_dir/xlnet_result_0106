No.:0
sentence: Hi, Iam 34years old & my sonography says Solid cystic complex cyst noted in 6 o clock position in retroareolar region in left breast measuring 19*18mm & left axillary lymphadenopathy.
input_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4036, 19, 35, 1199, 3422, 295, 23, 532, 1123, 94, 966, 6336, 349, 20975, 17, 19616, 556, 1881, 17, 19616, 1699, 25, 284, 17, 155, 5135, 740, 25, 20848, 4895, 155, 4225, 653, 25, 263, 4855, 10602, 1029, 8652, 1010, 2828, 1123, 263, 24, 469, 28107, 23201, 101, 1426, 24591, 9, 4, 3]
attention_masks:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
segment_ids:[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]


1
No.:0
sentence: hi, my doctor has come to the conclusion i have IBS after not showing any signs of chrons in a colonoscopy test.
input_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 2582, 19, 94, 2223, 51, 280, 22, 18, 5699, 17, 150, 47, 35, 7096, 99, 50, 2343, 124, 2832, 20, 17, 20276, 23, 25, 24, 10745, 28839, 934, 9, 4, 3]
attention_masks:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
segment_ids:[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]


0
No.:0
sentence: I have been a coffee drinker on and off since I was little.
input_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 35, 47, 72, 24, 2877, 3347, 118, 31, 21, 177, 196, 35, 30, 293, 9, 4, 3]
attention_masks:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
segment_ids:[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]


1
No.:0
sentence: hi, my doctor has come to the conclusion i have IBS after not showing any signs of chrons in a colonoscopy test.
input_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 2582, 19, 94, 2223, 51, 280, 22, 18, 5699, 17, 150, 47, 35, 7096, 99, 50, 2343, 124, 2832, 20, 17, 20276, 23, 25, 24, 10745, 28839, 934, 9, 4, 3]
attention_masks:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
segment_ids:[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]


2
No.:0
sentence: Hi, Iam 34years old & my sonography says Solid cystic complex cyst noted in 6 o clock position in retroareolar region in left breast measuring 19*18mm & left axillary lymphadenopathy.
input_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4036, 19, 35, 1199, 3422, 295, 23, 532, 1123, 94, 966, 6336, 349, 20975, 17, 19616, 556, 1881, 17, 19616, 1699, 25, 284, 17, 155, 5135, 740, 25, 20848, 4895, 155, 4225, 653, 25, 263, 4855, 10602, 1029, 8652, 1010, 2828, 1123, 263, 24, 469, 28107, 23201, 101, 1426, 24591, 9, 4, 3]
attention_masks:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
segment_ids:[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]


0
No.:0
sentence: I have been a coffee drinker on and off since I was little.
input_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 35, 47, 72, 24, 2877, 3347, 118, 31, 21, 177, 196, 35, 30, 293, 9, 4, 3]
attention_masks:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
segment_ids:[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]


0
======== Train Datasets: HIF2016_DIS00_polarity.tsv HIF2016_DIS01_polarity.tsv
======== Test Dataset: HIF2016_DIS02_polarity.tsv
========== Running training ==========
  Num examples = 2583
  Batch size = 32
  Num steps = 405
Train loss: 0.8756356067955494
Train loss: 0.560621640458703
Train loss: 0.39192313849925997
Train loss: 0.22484901524148881
Train loss: 0.14504469521343707
========== Running evaluation ==========
  Num examples =864
  Batch size = 32
========== Test results ==========
  eval_accuracy = 0.7465277777777778
  eval_loss = 1.1254717056398038
  loss = 0.14504469521343707
              precision    recall  f1-score   support

           0       0.62      0.84      0.71       171
           1       0.81      0.77      0.79       477
           2       0.74      0.63      0.68       216

   micro avg       0.75      0.75      0.75       864
   macro avg       0.72      0.75      0.73       864
weighted avg       0.76      0.75      0.75       864



len pred:  864
len true:  864
len sen:  864
======== Train Datasets: HIF2016_DIS02_polarity.tsv HIF2016_DIS01_polarity.tsv
======== Test Dataset: HIF2016_DIS00_polarity.tsv
========== Running training ==========
  Num examples = 2492
  Batch size = 32
  Num steps = 390
Train loss: 0.8389778926775053
Train loss: 0.5308093838877492
Train loss: 0.34104242369339066
Train loss: 0.20678607509894806
Train loss: 0.14800283240226955
========== Running evaluation ==========
  Num examples =955
  Batch size = 32
========== Test results ==========
  eval_accuracy = 0.7717277486910995
  eval_loss = 1.0280532856782278
  loss = 0.14800283240226955
              precision    recall  f1-score   support

           0       0.69      0.84      0.76       162
           1       0.81      0.79      0.80       499
           2       0.75      0.70      0.73       294

   micro avg       0.77      0.77      0.77       955
   macro avg       0.75      0.78      0.76       955
weighted avg       0.77      0.77      0.77       955



len pred:  1819
len true:  1819
len sen:  1819
======== Train Datasets: HIF2016_DIS02_polarity.tsv HIF2016_DIS00_polarity.tsv
======== Test Dataset: HIF2016_DIS01_polarity.tsv
========== Running training ==========
  Num examples = 1819
  Batch size = 32
  Num steps = 285
Train loss: 0.8974953325731414
Train loss: 0.5715652257204056
Train loss: 0.38027181210262434
Train loss: 0.2676525020173618
Train loss: 0.16098034847527742
========== Running evaluation ==========
  Num examples =1628
  Batch size = 32
========== Test results ==========
  eval_accuracy = 0.7463144963144963
  eval_loss = 1.0495074718606239
  loss = 0.16098034847527742
              precision    recall  f1-score   support

           0       0.86      0.54      0.67       302
           1       0.72      0.78      0.75       712
           2       0.74      0.81      0.77       614

   micro avg       0.75      0.75      0.75      1628
   macro avg       0.78      0.71      0.73      1628
weighted avg       0.76      0.75      0.74      1628



len pred:  3447
len true:  3447
len sen:  3447
======== Train Datasets: HIF2016_DIS02_factuality.tsv HIF2016_DIS00_factuality.tsv
======== Test Dataset: HIF2016_DIS01_factuality.tsv
========== Running training ==========
  Num examples = 1699
  Batch size = 32
  Num steps = 270
Train loss: 0.95977635316129
Train loss: 0.6174085112112873
Train loss: 0.4455730622669436
Train loss: 0.29855444015197036
Train loss: 0.18338809049916718
========== Running evaluation ==========
  Num examples =1593
  Batch size = 32
========== Test results ==========
  eval_accuracy = 0.807909604519774
  eval_loss = 0.7080945758521557
  loss = 0.18338809049916718
              precision    recall  f1-score   support

           0       0.93      0.85      0.89       931
           1       0.73      0.77      0.75       389
           2       0.60      0.71      0.65       273

   micro avg       0.81      0.81      0.81      1593
   macro avg       0.75      0.78      0.76      1593
weighted avg       0.82      0.81      0.81      1593



len pred:  1593
len true:  1593
len sen:  1593
======== Train Datasets: HIF2016_DIS01_factuality.tsv HIF2016_DIS00_factuality.tsv
======== Test Dataset: HIF2016_DIS02_factuality.tsv
========== Running training ==========
  Num examples = 2479
  Batch size = 32
  Num steps = 390
Train loss: 0.7707408991726962
Train loss: 0.4767685685451929
Train loss: 0.32295236223703855
Train loss: 0.2115019949322397
Train loss: 0.13434719473317072
========== Running evaluation ==========
  Num examples =813
  Batch size = 32
========== Test results ==========
  eval_accuracy = 0.7662976629766297
  eval_loss = 0.8395245745778084
  loss = 0.13434719473317072
              precision    recall  f1-score   support

           0       0.84      0.78      0.81       278
           1       0.76      0.82      0.79       310
           2       0.69      0.68      0.68       225

   micro avg       0.77      0.77      0.77       813
   macro avg       0.76      0.76      0.76       813
weighted avg       0.77      0.77      0.77       813



len pred:  2406
len true:  2406
len sen:  2406
======== Train Datasets: HIF2016_DIS01_factuality.tsv HIF2016_DIS02_factuality.tsv
======== Test Dataset: HIF2016_DIS00_factuality.tsv
========== Running training ==========
  Num examples = 2406
  Batch size = 32
  Num steps = 380
Train loss: 0.7107176033655802
Train loss: 0.46578908363978067
Train loss: 0.32099833607673645
Train loss: 0.21253425245483717
Train loss: 0.14032907928029695
========== Running evaluation ==========
  Num examples =886
  Batch size = 32
========== Test results ==========
  eval_accuracy = 0.7212189616252822
  eval_loss = 1.5046865663358144
  loss = 0.14032907928029695
              precision    recall  f1-score   support

           0       0.70      0.92      0.80       348
           1       0.75      0.70      0.72       271
           2       0.73      0.48      0.58       267

   micro avg       0.72      0.72      0.72       886
   macro avg       0.73      0.70      0.70       886
weighted avg       0.72      0.72      0.71       886



len pred:  3292
len true:  3292
len sen:  3292
p_array shape:  (3, 3447)
f_array shape:  (3, 3292)
new p_array shape:  (3, 850)
new f_array shape:  (3, 743)
error_m_row:  [3, 5, 3, 2, 3, 5, 0, 2, 1, 2, 0, 3, 5, 3, 0, 3, 0, 0, 0, 5, 3, 2, 2, 5, 3, 2, 0, 5, 0, 5, 3, 3, 3, 5, 0, 3, 3, 3, 2, 3, 5, 5, 3, 0, 0, 1, 3, 2, 3, 1, 5, 0, 1, 3, 3, 5, 3, 0, 3, 5, 4, 3, 5, 5, 0, 0, 0, 0, 5, 5, 0, 1, 0, 5, 0, 5, 0, 5, 3, 5, 3, 3, 5, 3, 3, 3, 3, 5, 0, 2, 4, 0, 5, 5, 3, 0, 3, 3, 5, 5, 3, 0, 3, 3, 5, 2, 2, 1, 3, 5, 0, 3, 3, 1, 1, 3, 3, 0, 5, 5, 2, 3, 5, 5, 3, 3, 3, 3, 2, 0, 5, 2, 2, 5, 5, 5, 5, 0, 2, 3, 3, 3, 2, 5, 5, 3, 5, 2, 3, 3, 3, 3, 3, 3, 0, 5, 0, 2, 3, 3, 2, 3, 5, 2, 5, 5, 5, 5, 2, 2, 4, 2, 5, 3, 5, 5, 2, 5, 5, 2, 2, 2, 2, 5, 3, 2, 2, 5, 5, 5, 3, 2, 4, 2, 3, 2, 3, 2, 2, 2, 2, 5, 5, 2]
error_m_column:  [5, 5, 2, 0, 4, 2, 1, 0, 2, 2, 5, 2, 5, 4, 2, 5, 5, 5, 3, 0, 2, 3, 3, 0, 0, 2, 1, 2, 3, 3, 3, 5, 1, 1, 4, 2, 5, 0, 5, 3, 3, 3, 5, 5, 5, 5, 3, 5, 1, 1, 1, 0, 3, 0, 1, 1, 1, 0, 4, 5, 2, 1, 3, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 3, 1, 5, 1, 1, 3, 1, 3, 2, 0, 5, 4, 5, 5, 5, 1, 2, 3, 0, 0, 4, 3, 0, 0, 1, 1, 1, 1, 0, 5, 2, 1, 4, 3, 1, 2, 5, 1, 0, 3, 0, 0, 5, 3, 1, 1, 4, 2, 5, 1, 1, 4, 4, 4, 2, 3, 3, 4, 2, 2, 2, 3, 4, 4, 4, 4, 2, 5, 4, 4, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 2, 2, 4, 5, 5, 2, 2, 0, 3, 2, 0, 2, 0, 0, 1, 3, 3, 5, 5, 5, 2, 3, 2, 4, 2, 1, 4, 2, 4, 1, 1, 0, 4, 3, 0, 5, 2, 2, 4, 0, 0, 0, 0, 3, 2, 0]
