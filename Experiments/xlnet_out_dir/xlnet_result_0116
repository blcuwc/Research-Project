======== Train Datasets: HIF2016_DIS00_polarity.tsv HIF2016_DIS01_polarity.tsv
======== Test Dataset: HIF2016_DIS02_polarity.tsv
========== Running training ==========
  Num examples = 2583
  Batch size = 32
  Num steps = 405
Train loss: 0.8405361175537109
Train loss: 0.49911952055990694
Train loss: 0.33170903474092484
Train loss: 0.20758919790387154
Train loss: 0.1584372263867408
========== Running evaluation ==========
  Num examples =864
  Batch size = 32
========== Test results ==========
  eval_accuracy = 0.75
  eval_loss = 1.0151026221337143
  loss = 0.1584372263867408
              precision    recall  f1-score   support

           0       0.69      0.78      0.73       171
           1       0.80      0.77      0.79       477
           2       0.69      0.68      0.68       216

   micro avg       0.75      0.75      0.75       864
   macro avg       0.73      0.74      0.73       864
weighted avg       0.75      0.75      0.75       864



len pred:  864
len true:  864
len sen:  864
======== Train Datasets: HIF2016_DIS02_polarity.tsv HIF2016_DIS01_polarity.tsv
======== Test Dataset: HIF2016_DIS00_polarity.tsv
========== Running training ==========
  Num examples = 2492
  Batch size = 32
  Num steps = 390
Train loss: 0.8661147909505027
Train loss: 0.5424738928869173
Train loss: 0.3684504688173145
Train loss: 0.2213246210829004
Train loss: 0.16813140976932142
========== Running evaluation ==========
  Num examples =955
  Batch size = 32
========== Test results ==========
  eval_accuracy = 0.7664921465968586
  eval_loss = 0.9950132628281911
  loss = 0.16813140976932142
              precision    recall  f1-score   support

           0       0.71      0.80      0.75       162
           1       0.78      0.84      0.81       499
           2       0.78      0.63      0.70       294

   micro avg       0.77      0.77      0.77       955
   macro avg       0.76      0.76      0.75       955
weighted avg       0.77      0.77      0.76       955



len pred:  1819
len true:  1819
len sen:  1819
======== Train Datasets: HIF2016_DIS02_polarity.tsv HIF2016_DIS00_polarity.tsv
======== Test Dataset: HIF2016_DIS01_polarity.tsv
========== Running training ==========
  Num examples = 1819
  Batch size = 32
  Num steps = 285
Train loss: 0.844287992055927
Train loss: 0.5205058992973396
Train loss: 0.35879471924688133
Train loss: 0.23432529205456376
Train loss: 0.1547334351842957
========== Running evaluation ==========
  Num examples =1628
  Batch size = 32
========== Test results ==========
  eval_accuracy = 0.7377149877149877
  eval_loss = 0.9843686678830315
  loss = 0.1547334351842957
              precision    recall  f1-score   support

           0       0.80      0.59      0.68       302
           1       0.70      0.79      0.74       712
           2       0.77      0.75      0.76       614

   micro avg       0.74      0.74      0.74      1628
   macro avg       0.76      0.71      0.73      1628
weighted avg       0.74      0.74      0.74      1628



len pred:  3447
len true:  3447
len sen:  3447
======== Train Datasets: HIF2016_DIS02_factuality.tsv HIF2016_DIS00_factuality.tsv
======== Test Dataset: HIF2016_DIS01_factuality.tsv
========== Running training ==========
  Num examples = 1699
  Batch size = 32
  Num steps = 270
Train loss: 0.9516493610616
Train loss: 0.6478978901539209
Train loss: 0.47059685835298504
Train loss: 0.3041610984869723
Train loss: 0.19214426599583537
========== Running evaluation ==========
  Num examples =1593
  Batch size = 32
========== Test results ==========
  eval_accuracy = 0.8148148148148148
  eval_loss = 0.6553796689212322
  loss = 0.19214426599583537
              precision    recall  f1-score   support

           0       0.93      0.88      0.90       931
           1       0.80      0.68      0.73       389
           2       0.57      0.79      0.66       273

   micro avg       0.81      0.81      0.81      1593
   macro avg       0.76      0.78      0.76      1593
weighted avg       0.83      0.81      0.82      1593



len pred:  1593
len true:  1593
len sen:  1593
======== Train Datasets: HIF2016_DIS01_factuality.tsv HIF2016_DIS00_factuality.tsv
======== Test Dataset: HIF2016_DIS02_factuality.tsv
========== Running training ==========
  Num examples = 2479
  Batch size = 32
  Num steps = 390
Train loss: 0.7251037080566605
Train loss: 0.4917464465289921
Train loss: 0.29656644726728465
Train loss: 0.19312110785153005
Train loss: 0.1384094985468047
========== Running evaluation ==========
  Num examples =813
  Batch size = 32
========== Test results ==========
  eval_accuracy = 0.7724477244772447
  eval_loss = 1.0234341873572423
  loss = 0.1384094985468047
              precision    recall  f1-score   support

           0       0.81      0.82      0.81       278
           1       0.75      0.86      0.80       310
           2       0.75      0.60      0.67       225

   micro avg       0.77      0.77      0.77       813
   macro avg       0.77      0.76      0.76       813
weighted avg       0.77      0.77      0.77       813



len pred:  2406
len true:  2406
len sen:  2406
======== Train Datasets: HIF2016_DIS01_factuality.tsv HIF2016_DIS02_factuality.tsv
======== Test Dataset: HIF2016_DIS00_factuality.tsv
========== Running training ==========
  Num examples = 2406
  Batch size = 32
  Num steps = 380
Train loss: 0.699877937634786
Train loss: 0.43011505603790284
Train loss: 0.28705851137638094
Train loss: 0.18985375752051672
Train loss: 0.12561262905597687
========== Running evaluation ==========
  Num examples =886
  Batch size = 32
========== Test results ==========
  eval_accuracy = 0.7268623024830699
  eval_loss = 1.1460708432963915
  loss = 0.12561262905597687
              precision    recall  f1-score   support

           0       0.77      0.87      0.82       348
           1       0.69      0.75      0.72       271
           2       0.69      0.52      0.59       267

   micro avg       0.73      0.73      0.73       886
   macro avg       0.72      0.71      0.71       886
weighted avg       0.72      0.73      0.72       886



len pred:  3292
len true:  3292
len sen:  3292
