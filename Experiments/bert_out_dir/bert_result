No.:0
sentence: Hi, Iam 34years old & my sonography says Solid cystic complex cyst noted in 6 o clock position in retroareolar region in left breast measuring 19*18mm & left axillary lymphadenopathy.
input_ids:[101, 8790, 117, 146, 2312, 3236, 4980, 7666, 1385, 111, 1139, 1488, 9543, 1867, 20375, 172, 6834, 2941, 2703, 172, 6834, 1204, 2382, 1107, 127, 184, 4705, 1700, 1107, 1231, 8005, 8836, 21459, 1805, 1107, 1286, 7209, 10099, 1627, 115, 1407, 6262, 111, 1286, 170, 8745, 12576, 1183, 181, 25698, 6397, 2728, 12233, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0]
attention_masks:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
segment_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]


No.:0
sentence: hi, my doctor has come to the conclusion i have IBS after not showing any signs of chrons in a colonoscopy test.
input_ids:[101, 20844, 117, 1139, 3995, 1144, 1435, 1106, 1103, 6593, 178, 1138, 146, 9782, 1170, 1136, 4000, 1251, 5300, 1104, 22572, 19298, 1107, 170, 1884, 4934, 2155, 20739, 2774, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
attention_masks:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
segment_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]


No.:0
sentence: I have been a coffee drinker on and off since I was little.
input_ids:[101, 146, 1138, 1151, 170, 3538, 3668, 1200, 1113, 1105, 1228, 1290, 146, 1108, 1376, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
attention_masks:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
segment_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]


No.:0
sentence: hi, my doctor has come to the conclusion i have IBS after not showing any signs of chrons in a colonoscopy test.
input_ids:[101, 20844, 117, 1139, 3995, 1144, 1435, 1106, 1103, 6593, 178, 1138, 146, 9782, 1170, 1136, 4000, 1251, 5300, 1104, 22572, 19298, 1107, 170, 1884, 4934, 2155, 20739, 2774, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
attention_masks:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
segment_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]


No.:0
sentence: Hi, Iam 34years old & my sonography says Solid cystic complex cyst noted in 6 o clock position in retroareolar region in left breast measuring 19*18mm & left axillary lymphadenopathy.
input_ids:[101, 8790, 117, 146, 2312, 3236, 4980, 7666, 1385, 111, 1139, 1488, 9543, 1867, 20375, 172, 6834, 2941, 2703, 172, 6834, 1204, 2382, 1107, 127, 184, 4705, 1700, 1107, 1231, 8005, 8836, 21459, 1805, 1107, 1286, 7209, 10099, 1627, 115, 1407, 6262, 111, 1286, 170, 8745, 12576, 1183, 181, 25698, 6397, 2728, 12233, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0]
attention_masks:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
segment_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]


No.:0
sentence: I have been a coffee drinker on and off since I was little.
input_ids:[101, 146, 1138, 1151, 170, 3538, 3668, 1200, 1113, 1105, 1228, 1290, 146, 1108, 1376, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
attention_masks:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
segment_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]


======== Train Datasets: HIF2016_DIS00_polarity.tsv HIF2016_DIS01_polarity.tsv
======== Test Dataset: HIF2016_DIS02_polarity.tsv
========== Running training ==========
  Num examples = 2583
  Batch size = 32
  Num steps = 405
Train loss: 0.9528558850288391
Train loss: 0.699307705461979
Train loss: 0.5400518476963043
Train loss: 0.4653101865202188
Train loss: 0.43643702659755945
========== Running evaluation ==========
  Num examples =864
  Batch size = 32
========== Test results ==========
  eval_accuracy = 0.75
  eval_loss = 0.6240585622964082
  loss = 0.43643702659755945
              precision    recall  f1-score   support

           0       0.80      0.64      0.71       171
           1       0.75      0.86      0.80       477
           2       0.71      0.60      0.65       216

   micro avg       0.75      0.75      0.75       864
   macro avg       0.75      0.70      0.72       864
weighted avg       0.75      0.75      0.75       864



======== Train Datasets: HIF2016_DIS02_polarity.tsv HIF2016_DIS01_polarity.tsv
======== Test Dataset: HIF2016_DIS00_polarity.tsv
========== Running training ==========
  Num examples = 2492
  Batch size = 32
  Num steps = 390
Train loss: 1.0045258062226432
Train loss: 0.7089294171952581
Train loss: 0.5472176152390319
Train loss: 0.464963710927344
Train loss: 0.4174158718291815
========== Running evaluation ==========
  Num examples =955
  Batch size = 32
========== Test results ==========
  eval_accuracy = 0.7319371727748691
  eval_loss = 0.6604453583558401
  loss = 0.4174158718291815
              precision    recall  f1-score   support

           0       0.61      0.81      0.69       162
           1       0.80      0.75      0.77       499
           2       0.72      0.67      0.69       294

   micro avg       0.73      0.73      0.73       955
   macro avg       0.71      0.74      0.72       955
weighted avg       0.74      0.73      0.73       955



======== Train Datasets: HIF2016_DIS02_polarity.tsv HIF2016_DIS00_polarity.tsv
======== Test Dataset: HIF2016_DIS01_polarity.tsv
========== Running training ==========
  Num examples = 1819
  Batch size = 32
  Num steps = 285
Train loss: 1.0512714801090104
Train loss: 0.8444642541663987
Train loss: 0.5987424296992165
Train loss: 0.47654237917491366
Train loss: 0.3820132418934788
========== Running evaluation ==========
  Num examples =1628
  Batch size = 32
========== Test results ==========
  eval_accuracy = 0.7358722358722358
  eval_loss = 0.693752110004425
  loss = 0.3820132418934788
              precision    recall  f1-score   support

           0       0.78      0.63      0.69       302
           1       0.73      0.77      0.75       712
           2       0.73      0.75      0.74       614

   micro avg       0.74      0.74      0.74      1628
   macro avg       0.74      0.72      0.73      1628
weighted avg       0.74      0.74      0.73      1628



======== Train Datasets: HIF2016_DIS02_factuality.tsv HIF2016_DIS00_factuality.tsv
======== Test Dataset: HIF2016_DIS01_factuality.tsv
========== Running training ==========
  Num examples = 1699
  Batch size = 32
  Num steps = 270
Train loss: 1.0035881793723915
Train loss: 0.787407249774573
Train loss: 0.6769442946281073
Train loss: 0.5927823849444119
Train loss: 0.5100179095313234
========== Running evaluation ==========
  Num examples =1593
  Batch size = 32
========== Test results ==========
  eval_accuracy = 0.8016321406151915
  eval_loss = 0.526248921751976
  loss = 0.5100179095313234
              precision    recall  f1-score   support

           0       0.91      0.88      0.89       931
           1       0.64      0.76      0.70       389
           2       0.69      0.62      0.65       273

   micro avg       0.80      0.80      0.80      1593
   macro avg       0.75      0.75      0.75      1593
weighted avg       0.81      0.80      0.80      1593



======== Train Datasets: HIF2016_DIS01_factuality.tsv HIF2016_DIS00_factuality.tsv
======== Test Dataset: HIF2016_DIS02_factuality.tsv
========== Running training ==========
  Num examples = 2479
  Batch size = 32
  Num steps = 390
Train loss: 0.8883365783598516
Train loss: 0.5788346130352515
Train loss: 0.49694417436401567
Train loss: 0.4513489484012901
Train loss: 0.400455075424987
========== Running evaluation ==========
  Num examples =813
  Batch size = 32
========== Test results ==========
  eval_accuracy = 0.7207872078720787
  eval_loss = 0.7201251605382333
  loss = 0.400455075424987
              precision    recall  f1-score   support

           0       0.80      0.78      0.79       278
           1       0.65      0.91      0.76       310
           2       0.84      0.39      0.53       225

   micro avg       0.72      0.72      0.72       813
   macro avg       0.76      0.69      0.69       813
weighted avg       0.75      0.72      0.70       813



======== Train Datasets: HIF2016_DIS01_factuality.tsv HIF2016_DIS02_factuality.tsv
======== Test Dataset: HIF2016_DIS00_factuality.tsv
========== Running training ==========
  Num examples = 2406
  Batch size = 32
  Num steps = 380
Train loss: 0.9111162614822388
Train loss: 0.5620111989974975
Train loss: 0.4623197841644287
Train loss: 0.3953076511621475
Train loss: 0.34599630534648895
========== Running evaluation ==========
  Num examples =886
  Batch size = 32
========== Test results ==========
  eval_accuracy = 0.7099322799097065
  eval_loss = 0.7430063113570213
  loss = 0.34599630534648895
              precision    recall  f1-score   support

           0       0.75      0.86      0.80       348
           1       0.67      0.74      0.70       271
           2       0.70      0.48      0.57       267

   micro avg       0.71      0.71      0.71       886
   macro avg       0.70      0.69      0.69       886
weighted avg       0.71      0.71      0.70       886



