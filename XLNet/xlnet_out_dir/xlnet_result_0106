No.:0
sentence: Hi, Iam 34years old & my sonography says Solid cystic complex cyst noted in 6 o clock position in retroareolar region in left breast measuring 19*18mm & left axillary lymphadenopathy.
input_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4036, 19, 35, 1199, 3422, 295, 23, 532, 1123, 94, 966, 6336, 349, 20975, 17, 19616, 556, 1881, 17, 19616, 1699, 25, 284, 17, 155, 5135, 740, 25, 20848, 4895, 155, 4225, 653, 25, 263, 4855, 10602, 1029, 8652, 1010, 2828, 1123, 263, 24, 469, 28107, 23201, 101, 1426, 24591, 9, 4, 3]
attention_masks:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
segment_ids:[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]


1
No.:0
sentence: hi, my doctor has come to the conclusion i have IBS after not showing any signs of chrons in a colonoscopy test.
input_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 2582, 19, 94, 2223, 51, 280, 22, 18, 5699, 17, 150, 47, 35, 7096, 99, 50, 2343, 124, 2832, 20, 17, 20276, 23, 25, 24, 10745, 28839, 934, 9, 4, 3]
attention_masks:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
segment_ids:[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]


0
No.:0
sentence: I have been a coffee drinker on and off since I was little.
input_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 35, 47, 72, 24, 2877, 3347, 118, 31, 21, 177, 196, 35, 30, 293, 9, 4, 3]
attention_masks:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
segment_ids:[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]


1
No.:0
sentence: hi, my doctor has come to the conclusion i have IBS after not showing any signs of chrons in a colonoscopy test.
input_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 17, 2582, 19, 94, 2223, 51, 280, 22, 18, 5699, 17, 150, 47, 35, 7096, 99, 50, 2343, 124, 2832, 20, 17, 20276, 23, 25, 24, 10745, 28839, 934, 9, 4, 3]
attention_masks:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
segment_ids:[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]


2
No.:0
sentence: Hi, Iam 34years old & my sonography says Solid cystic complex cyst noted in 6 o clock position in retroareolar region in left breast measuring 19*18mm & left axillary lymphadenopathy.
input_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4036, 19, 35, 1199, 3422, 295, 23, 532, 1123, 94, 966, 6336, 349, 20975, 17, 19616, 556, 1881, 17, 19616, 1699, 25, 284, 17, 155, 5135, 740, 25, 20848, 4895, 155, 4225, 653, 25, 263, 4855, 10602, 1029, 8652, 1010, 2828, 1123, 263, 24, 469, 28107, 23201, 101, 1426, 24591, 9, 4, 3]
attention_masks:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
segment_ids:[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]


0
No.:0
sentence: I have been a coffee drinker on and off since I was little.
input_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 35, 47, 72, 24, 2877, 3347, 118, 31, 21, 177, 196, 35, 30, 293, 9, 4, 3]
attention_masks:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
segment_ids:[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]


0
======== Train Datasets: HIF2016_DIS00_polarity.tsv HIF2016_DIS01_polarity.tsv
======== Test Dataset: HIF2016_DIS02_polarity.tsv
========== Running training ==========
  Num examples = 2583
  Batch size = 32
  Num steps = 405
Train loss: 0.8519740112125873
Train loss: 0.5167470315471292
Train loss: 0.3444236398674548
Train loss: 0.21104278969578444
Train loss: 0.13782600592821836
========== Running evaluation ==========
  Num examples =864
  Batch size = 32
========== Test results ==========
  eval_accuracy = 0.7418981481481481
  eval_loss = 1.1657679467289537
  loss = 0.13782600592821836
              precision    recall  f1-score   support

           0       0.60      0.85      0.70       171
           1       0.83      0.70      0.76       477
           2       0.73      0.74      0.74       216

   micro avg       0.74      0.74      0.74       864
   macro avg       0.72      0.76      0.73       864
weighted avg       0.76      0.74      0.74       864



len pred:  864
len true:  864
len sen:  864
======== Train Datasets: HIF2016_DIS02_polarity.tsv HIF2016_DIS01_polarity.tsv
======== Test Dataset: HIF2016_DIS00_polarity.tsv
========== Running training ==========
  Num examples = 2492
  Batch size = 32
  Num steps = 390
Train loss: 0.8371147792060654
Train loss: 0.5329490532348682
Train loss: 0.3690714629052521
Train loss: 0.23172248464513134
Train loss: 0.15217943489551544
========== Running evaluation ==========
  Num examples =955
  Batch size = 32
========== Test results ==========
  eval_accuracy = 0.7790575916230367
  eval_loss = 0.8764062573512396
  loss = 0.15217943489551544
              precision    recall  f1-score   support

           0       0.74      0.83      0.78       162
           1       0.83      0.78      0.81       499
           2       0.72      0.75      0.74       294

   micro avg       0.78      0.78      0.78       955
   macro avg       0.76      0.79      0.77       955
weighted avg       0.78      0.78      0.78       955



len pred:  1819
len true:  1819
len sen:  1819
======== Train Datasets: HIF2016_DIS02_polarity.tsv HIF2016_DIS00_polarity.tsv
======== Test Dataset: HIF2016_DIS01_polarity.tsv
========== Running training ==========
  Num examples = 1819
  Batch size = 32
  Num steps = 285
Train loss: 0.8822549964700427
Train loss: 0.5557217906628337
Train loss: 0.3896550927311182
Train loss: 0.24300029880500265
Train loss: 0.15227972217170255
========== Running evaluation ==========
  Num examples =1628
  Batch size = 32
========== Test results ==========
  eval_accuracy = 0.754914004914005
  eval_loss = 0.9451925970760047
  loss = 0.15227972217170255
              precision    recall  f1-score   support

           0       0.73      0.72      0.72       302
           1       0.77      0.72      0.74       712
           2       0.75      0.81      0.78       614

   micro avg       0.75      0.75      0.75      1628
   macro avg       0.75      0.75      0.75      1628
weighted avg       0.76      0.75      0.75      1628



len pred:  3447
len true:  3447
len sen:  3447
======== Train Datasets: HIF2016_DIS02_factuality.tsv HIF2016_DIS00_factuality.tsv
======== Test Dataset: HIF2016_DIS01_factuality.tsv
========== Running training ==========
  Num examples = 1699
  Batch size = 32
  Num steps = 270
Train loss: 0.8788058083012419
Train loss: 0.6042291881903162
Train loss: 0.44636633092502376
Train loss: 0.2737698067893397
Train loss: 0.17014197007102785
========== Running evaluation ==========
  Num examples =1593
  Batch size = 32
========== Test results ==========
  eval_accuracy = 0.8141870684243565
  eval_loss = 0.6914901387691498
  loss = 0.17014197007102785
              precision    recall  f1-score   support

           0       0.89      0.92      0.91       931
           1       0.80      0.64      0.71       389
           2       0.60      0.70      0.64       273

   micro avg       0.81      0.81      0.81      1593
   macro avg       0.76      0.75      0.75      1593
weighted avg       0.82      0.81      0.81      1593



len pred:  1593
len true:  1593
len sen:  1593
======== Train Datasets: HIF2016_DIS01_factuality.tsv HIF2016_DIS00_factuality.tsv
======== Test Dataset: HIF2016_DIS02_factuality.tsv
========== Running training ==========
  Num examples = 2479
  Batch size = 32
  Num steps = 390
Train loss: 0.737281065869641
Train loss: 0.46286135408785434
Train loss: 0.2921928515681973
Train loss: 0.19454713489908676
Train loss: 0.12118807901906503
========== Running evaluation ==========
  Num examples =813
  Batch size = 32
========== Test results ==========
  eval_accuracy = 0.7380073800738007
  eval_loss = 1.2530890743319805
  loss = 0.12118807901906503
              precision    recall  f1-score   support

           0       0.81      0.81      0.81       278
           1       0.68      0.87      0.76       310
           2       0.75      0.47      0.58       225

   micro avg       0.74      0.74      0.74       813
   macro avg       0.75      0.72      0.72       813
weighted avg       0.75      0.74      0.73       813



len pred:  2406
len true:  2406
len sen:  2406
======== Train Datasets: HIF2016_DIS01_factuality.tsv HIF2016_DIS02_factuality.tsv
======== Test Dataset: HIF2016_DIS00_factuality.tsv
========== Running training ==========
  Num examples = 2406
  Batch size = 32
  Num steps = 380
Train loss: 0.7006621619065603
Train loss: 0.4320893273750941
Train loss: 0.297055058379968
Train loss: 0.18710697074731192
Train loss: 0.11385486721992492
========== Running evaluation ==========
  Num examples =886
  Batch size = 32
========== Test results ==========
  eval_accuracy = 0.7178329571106095
  eval_loss = 1.3340239099093847
  loss = 0.11385486721992492
              precision    recall  f1-score   support

           0       0.78      0.85      0.81       348
           1       0.67      0.81      0.73       271
           2       0.68      0.45      0.54       267

   micro avg       0.72      0.72      0.72       886
   macro avg       0.71      0.70      0.70       886
weighted avg       0.72      0.72      0.71       886



len pred:  3292
len true:  3292
len sen:  3292
p_array shape:  (3, 3447)
f_array shape:  (3, 3292)
new p_array shape:  (3, 833)
new f_array shape:  (3, 759)
error_m_row:  [1, 5, 3, 5, 2, 3, 3, 5, 0, 0, 0, 2, 5, 2, 5, 5, 0, 5, 3, 5, 3, 3, 3, 5, 0, 0, 5, 0, 5, 0, 2, 0, 5, 3, 5, 0, 5, 2, 5, 3, 3, 0, 0, 0, 3, 5, 3, 5, 1, 5, 3, 5, 0, 1, 3, 0, 3, 5, 0, 1, 5, 5, 2, 0, 3, 5, 4, 5, 5, 5, 5, 0, 2, 0, 5, 0, 3, 1, 0, 5, 3, 0, 5, 5, 5, 3, 5, 0, 5, 3, 5, 3, 3, 0, 5, 0, 5, 3, 5, 5, 5, 3, 3, 2, 2, 1, 3, 5, 5, 5, 5, 5, 1, 5, 3, 3, 5, 2, 3, 3, 3, 0, 2, 5, 3, 2, 3, 3, 2, 0, 0, 5, 3, 4, 0, 5, 0, 5, 0, 3, 3, 3, 2, 5, 0, 3, 5, 2, 0, 3, 3, 5, 3, 5, 0, 5, 2, 3, 5, 5, 5, 5, 5, 1, 2, 5, 3, 3, 3, 0, 0, 5, 2, 0, 2, 3, 5, 2, 0, 5, 3, 5, 2, 2, 5, 0, 3, 5, 5, 5, 5, 2, 0, 4, 4, 3, 5, 2, 5, 5, 5, 5, 5, 3, 3]
error_m_column:  [0, 3, 2, 3, 0, 2, 2, 2, 0, 2, 1, 0, 3, 2, 0, 1, 4, 5, 3, 3, 4, 0, 3, 3, 0, 3, 0, 3, 1, 3, 3, 3, 2, 3, 3, 5, 3, 3, 2, 5, 3, 3, 4, 2, 2, 3, 1, 3, 3, 3, 0, 2, 3, 5, 3, 3, 3, 1, 0, 3, 1, 1, 1, 0, 4, 2, 2, 1, 1, 3, 3, 1, 5, 3, 2, 1, 5, 0, 1, 1, 4, 3, 1, 3, 1, 4, 1, 3, 1, 3, 5, 2, 5, 4, 4, 5, 1, 2, 3, 3, 0, 1, 4, 4, 2, 3, 2, 3, 3, 4, 5, 3, 3, 3, 3, 2, 1, 1, 1, 3, 1, 2, 3, 5, 5, 4, 5, 5, 0, 2, 2, 1, 4, 0, 3, 2, 5, 4, 4, 4, 4, 4, 3, 5, 4, 4, 5, 5, 1, 1, 3, 1, 3, 1, 3, 3, 0, 5, 0, 5, 0, 0, 4, 4, 2, 4, 2, 1, 3, 1, 3, 5, 5, 2, 4, 1, 0, 2, 5, 1, 4, 2, 1, 2, 5, 1, 3, 1, 1, 4, 0, 3, 0, 0, 5, 4, 1, 5, 2, 2, 2, 2, 1, 0, 1]
