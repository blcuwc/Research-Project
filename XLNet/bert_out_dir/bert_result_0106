No.:0
sentence: Hi, Iam 34years old & my sonography says Solid cystic complex cyst noted in 6 o clock position in retroareolar region in left breast measuring 19*18mm & left axillary lymphadenopathy.
input_ids:[101, 8790, 117, 146, 2312, 3236, 4980, 7666, 1385, 111, 1139, 1488, 9543, 1867, 20375, 172, 6834, 2941, 2703, 172, 6834, 1204, 2382, 1107, 127, 184, 4705, 1700, 1107, 1231, 8005, 8836, 21459, 1805, 1107, 1286, 7209, 10099, 1627, 115, 1407, 6262, 111, 1286, 170, 8745, 12576, 1183, 181, 25698, 6397, 2728, 12233, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0]
attention_masks:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
segment_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]


No.:0
sentence: hi, my doctor has come to the conclusion i have IBS after not showing any signs of chrons in a colonoscopy test.
input_ids:[101, 20844, 117, 1139, 3995, 1144, 1435, 1106, 1103, 6593, 178, 1138, 146, 9782, 1170, 1136, 4000, 1251, 5300, 1104, 22572, 19298, 1107, 170, 1884, 4934, 2155, 20739, 2774, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
attention_masks:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
segment_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]


No.:0
sentence: I have been a coffee drinker on and off since I was little.
input_ids:[101, 146, 1138, 1151, 170, 3538, 3668, 1200, 1113, 1105, 1228, 1290, 146, 1108, 1376, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
attention_masks:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
segment_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]


No.:0
sentence: hi, my doctor has come to the conclusion i have IBS after not showing any signs of chrons in a colonoscopy test.
input_ids:[101, 20844, 117, 1139, 3995, 1144, 1435, 1106, 1103, 6593, 178, 1138, 146, 9782, 1170, 1136, 4000, 1251, 5300, 1104, 22572, 19298, 1107, 170, 1884, 4934, 2155, 20739, 2774, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
attention_masks:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
segment_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]


No.:0
sentence: Hi, Iam 34years old & my sonography says Solid cystic complex cyst noted in 6 o clock position in retroareolar region in left breast measuring 19*18mm & left axillary lymphadenopathy.
input_ids:[101, 8790, 117, 146, 2312, 3236, 4980, 7666, 1385, 111, 1139, 1488, 9543, 1867, 20375, 172, 6834, 2941, 2703, 172, 6834, 1204, 2382, 1107, 127, 184, 4705, 1700, 1107, 1231, 8005, 8836, 21459, 1805, 1107, 1286, 7209, 10099, 1627, 115, 1407, 6262, 111, 1286, 170, 8745, 12576, 1183, 181, 25698, 6397, 2728, 12233, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0]
attention_masks:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
segment_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]


No.:0
sentence: I have been a coffee drinker on and off since I was little.
input_ids:[101, 146, 1138, 1151, 170, 3538, 3668, 1200, 1113, 1105, 1228, 1290, 146, 1108, 1376, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
attention_masks:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
segment_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]


======== Train Datasets: HIF2016_DIS00_polarity.tsv HIF2016_DIS01_polarity.tsv
======== Test Dataset: HIF2016_DIS02_polarity.tsv
========== Running training ==========
  Num examples = 2583
  Batch size = 32
  Num steps = 405
Train loss: 0.9518939681351185
Train loss: 0.7202699437737465
Train loss: 0.5730930117890238
Train loss: 0.4926930733025074
Train loss: 0.44261355716735123
========== Running evaluation ==========
  Num examples =864
  Batch size = 32
========== Test results ==========
  eval_accuracy = 0.7129629629629629
  eval_loss = 0.6988591110264813
  loss = 0.44261355716735123
              precision    recall  f1-score   support

           0       0.84      0.47      0.60       171
           1       0.69      0.91      0.78       477
           2       0.75      0.48      0.59       216

   micro avg       0.71      0.71      0.71       864
   macro avg       0.76      0.62      0.66       864
weighted avg       0.73      0.71      0.70       864



len pred:  864
len true:  864
len sen:  864
======== Train Datasets: HIF2016_DIS02_polarity.tsv HIF2016_DIS01_polarity.tsv
======== Test Dataset: HIF2016_DIS00_polarity.tsv
========== Running training ==========
  Num examples = 2492
  Batch size = 32
  Num steps = 390
Train loss: 0.984451109712774
Train loss: 0.7438079409010998
Train loss: 0.5662556333201272
Train loss: 0.49323706193403766
Train loss: 0.44720691345728836
========== Running evaluation ==========
  Num examples =955
  Batch size = 32
========== Test results ==========
  eval_accuracy = 0.749738219895288
  eval_loss = 0.6499651511510213
  loss = 0.44720691345728836
              precision    recall  f1-score   support

           0       0.66      0.78      0.72       162
           1       0.77      0.83      0.80       499
           2       0.76      0.61      0.68       294

   micro avg       0.75      0.75      0.75       955
   macro avg       0.73      0.74      0.73       955
weighted avg       0.75      0.75      0.75       955



len pred:  1819
len true:  1819
len sen:  1819
======== Train Datasets: HIF2016_DIS02_polarity.tsv HIF2016_DIS00_polarity.tsv
======== Test Dataset: HIF2016_DIS01_polarity.tsv
========== Running training ==========
  Num examples = 1819
  Batch size = 32
  Num steps = 285
Train loss: 0.9809530515755925
Train loss: 0.8130799861890929
Train loss: 0.6323374951524394
Train loss: 0.5005238311631339
Train loss: 0.4283726630466325
========== Running evaluation ==========
  Num examples =1628
  Batch size = 32
========== Test results ==========
  eval_accuracy = 0.7223587223587223
  eval_loss = 0.6612186992869657
  loss = 0.4283726630466325
              precision    recall  f1-score   support

           0       0.63      0.74      0.68       302
           1       0.77      0.71      0.74       712
           2       0.72      0.73      0.72       614

   micro avg       0.72      0.72      0.72      1628
   macro avg       0.71      0.72      0.71      1628
weighted avg       0.73      0.72      0.72      1628



len pred:  3447
len true:  3447
len sen:  3447
======== Train Datasets: HIF2016_DIS02_factuality.tsv HIF2016_DIS00_factuality.tsv
======== Test Dataset: HIF2016_DIS01_factuality.tsv
========== Running training ==========
  Num examples = 1699
  Batch size = 32
  Num steps = 270
Train loss: 1.0489621319860782
Train loss: 0.8597200940240104
Train loss: 0.6730831906480609
Train loss: 0.5695024239567091
Train loss: 0.4797347620973047
========== Running evaluation ==========
  Num examples =1593
  Batch size = 32
========== Test results ==========
  eval_accuracy = 0.7576898932831136
  eval_loss = 0.5762407803535461
  loss = 0.4797347620973047
              precision    recall  f1-score   support

           0       0.92      0.84      0.88       931
           1       0.82      0.51      0.63       389
           2       0.46      0.84      0.59       273

   micro avg       0.76      0.76      0.76      1593
   macro avg       0.73      0.73      0.70      1593
weighted avg       0.82      0.76      0.77      1593



len pred:  1593
len true:  1593
len sen:  1593
======== Train Datasets: HIF2016_DIS01_factuality.tsv HIF2016_DIS00_factuality.tsv
======== Test Dataset: HIF2016_DIS02_factuality.tsv
========== Running training ==========
  Num examples = 2479
  Batch size = 32
  Num steps = 390
Train loss: 0.964072784046074
Train loss: 0.6120691821946727
Train loss: 0.45507672970945184
Train loss: 0.3955876573726728
Train loss: 0.3513740037942862
========== Running evaluation ==========
  Num examples =813
  Batch size = 32
========== Test results ==========
  eval_accuracy = 0.7183271832718328
  eval_loss = 0.7315940386973895
  loss = 0.3513740037942862
              precision    recall  f1-score   support

           0       0.87      0.71      0.78       278
           1       0.63      0.92      0.75       310
           2       0.74      0.45      0.56       225

   micro avg       0.72      0.72      0.72       813
   macro avg       0.75      0.69      0.70       813
weighted avg       0.74      0.72      0.71       813



len pred:  2406
len true:  2406
len sen:  2406
======== Train Datasets: HIF2016_DIS01_factuality.tsv HIF2016_DIS02_factuality.tsv
======== Test Dataset: HIF2016_DIS00_factuality.tsv
========== Running training ==========
  Num examples = 2406
  Batch size = 32
  Num steps = 380
Train loss: 0.9292075355847677
Train loss: 0.6315474998950958
Train loss: 0.5108470169703165
Train loss: 0.46904372692108154
Train loss: 0.42481889406840007
========== Running evaluation ==========
  Num examples =886
  Batch size = 32
========== Test results ==========
  eval_accuracy = 0.7020316027088036
  eval_loss = 0.8657309104289327
  loss = 0.42481889406840007
              precision    recall  f1-score   support

           0       0.69      0.93      0.79       348
           1       0.68      0.76      0.72       271
           2       0.82      0.34      0.49       267

   micro avg       0.70      0.70      0.70       886
   macro avg       0.73      0.68      0.66       886
weighted avg       0.73      0.70      0.68       886



len pred:  3292
len true:  3292
len sen:  3292
